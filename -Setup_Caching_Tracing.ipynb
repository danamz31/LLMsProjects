{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d083e4cf-077d-47aa-8828-b49a29ae6c1a",
   "metadata": {},
   "source": [
    "My Piece of Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2506d0fa-1a9a-4f82-8f02-888f5a325ce6",
   "metadata": {},
   "source": [
    "# Set UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6ea3eef-28eb-404c-8c99-a54b69407e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "from IPython.display import Markdown, display\n",
    "from langfuse.openai import AsyncOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "client = AsyncOpenAI()\n",
    "\n",
    "# from langfuse.openai import openai\n",
    "# from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5ca8dbe-8f38-4542-a908-663a4eb71fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _msg(role, content): #returns a dict\n",
    "    return {'role':role, 'content':content}\n",
    "\n",
    "def user(content):\n",
    "    return _msg('user', content)\n",
    "\n",
    "def system(content):\n",
    "    return _msg('system', content)\n",
    "\n",
    "def assistant(content):\n",
    "    return _msg('assistant', content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93852d2-42bb-4d63-b48b-b064c5dc19d3",
   "metadata": {},
   "source": [
    "# Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "742b1f28-b565-4ebf-a933-36d6e8c14ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diskcache import Cache\n",
    "# cache = Cache() --> this is a temporary cache where it's not saved when the program is restart\n",
    "cache = Cache(directory = '.cache_coures_dana') # This is not a temporary where tha cache is saved in a directory\n",
    "                                                # and can be accessed any time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d0eb71f-71b4-4d44-959e-a606e425bf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the caching get and set Asyc:\n",
    "# wrapper thing\n",
    "# when making async function: 'async' key word, when calling it, 'await' key word\n",
    "import asyncio\n",
    "async def set_async_cache(key, val, **kwargs): # needs key and value, and it can take other args by their key words(kwargs)\n",
    "    # return sth that is awiat\n",
    "    return await asyncio.to_thread(cache.set, key, val, **kwargs)\n",
    "\n",
    "async def get_async_cache(key, **kwargs): # needs key, kwargs, and a default can be passed with kwargs\n",
    "    return await asyncio.to_thread(cache.get, key, **kwargs)\n",
    "\n",
    "# to call async func must be await"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd7ceec-ec6f-4e31-b822-ccfb85a0e1a6",
   "metadata": {},
   "source": [
    "### Make the cache key with hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54b0294f-a83a-4c4a-adfa-74ea1db66a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashlib import md5\n",
    "import json\n",
    "# print(md5(b'dana the one').hexdigest())\n",
    "# md5 works with only str\n",
    "\n",
    "def cache_key(key, **kwargs):\n",
    "    kwargs_str = json.dumps(kwargs, sort_keys = True)\n",
    "    kwargs_hashed = md5(kwargs_str.encode('utf-8')).hexdigest()\n",
    "    hashed_key = f'{key}__{kwargs_hashed}'\n",
    "    return hashed_key\n",
    "\n",
    "def _chat_completion_cache_key(*, model, messages, **kwargs): # the first * to make the function only accepts key words args\n",
    "    return cache_key('openai_chat_completion', model = model, messages = messages, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "988be06a-65ce-4316-849d-0995ba6ed823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.chat import ChatCompletion\n",
    "from functools import update_wrapper\n",
    "CACHE_MISS_SENTINEL = object() # we need to create sth that can be created ONLY once and can't be created accidentally anytime, like a memory address => an object\n",
    "\n",
    "async def cached_chat_completion(*, model, messages, **kwargs) -> ChatCompletion : # the 'async' because we want to use async functions\n",
    "    # MAKE KEY\n",
    "    cache_key = _chat_completion_cache_key(model = model, messages = messages, **kwargs)\n",
    "    cached_value = await get_async_cache(cache_key, default = CACHE_MISS_SENTINEL) # we will use this as default when the key can't be found in the cache\n",
    "    \n",
    "    # CACHE MISS\n",
    "    if cached_value is CACHE_MISS_SENTINEL:\n",
    "        completion = await client.chat.completions.create(model = model, messages = messages, **kwargs) # a) make openai call\n",
    "        await set_async_cache(cache_key, completion.model_dump_json()) # b) set the output in the cache\n",
    "        return completion # this is a chat completion type\n",
    "\n",
    "    # CACHE HIT\n",
    "    else:\n",
    "        return ChatCompletion.model_validate(json.loads(cached_value)) # Cached Value is the vlue we need\n",
    "\n",
    "cached_chat_completion = update_wrapper(cached_chat_completion, client.chat.completions.create) # for auto completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "225f2d47-7280-4179-94bd-448a6b25df65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-BX2E6DWKucjXIdxDDjabcf6rrY97C', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='\"Hello\" in German is \"Hallo.\"', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1747213082, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_dbaca60df0', usage=CompletionUsage(completion_tokens=10, prompt_tokens=15, total_tokens=25, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion = await cached_chat_completion(\n",
    "    messages = [user('What is \"Hello\" in German?')],\n",
    "    model = 'gpt-4o-mini'\n",
    ")\n",
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "061e3790-b870-47e0-9951-5991835df3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-BX2E6DWKucjXIdxDDjabcf6rrY97C', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='\"Hello\" in German is \"Hallo.\"', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1747213082, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_dbaca60df0', usage=CompletionUsage(completion_tokens=10, prompt_tokens=15, total_tokens=25, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion = await cached_chat_completion(\n",
    "    messages = [user('What is \"Hello\" in German?')],\n",
    "    model = 'gpt-4o-mini'\n",
    ")\n",
    "completion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
